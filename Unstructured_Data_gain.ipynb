{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9efaf4a5-36df-4025-9a7c-fce5e43c89c1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "# Advanced Pandas II — Unstructured Data\n",
    "\n",
    "Modern data is rarely a clean table. It is multimodal: numbers, text, images, and nested JSON metadata.  \n",
    "To analyze it efficiently and prepare it for machine learning, we must master data loading, normalization, and feature extraction across different formats.\n",
    "\n",
    "In this notebook you will work with multiple real-world datasets including retail transactions, company metadata, and real estate listings to explore how pandas and related tools handle structured, semi-structured, textual, and visual data.\n",
    "\n",
    "</div>\n",
    "\n",
    "### What you will do\n",
    "\n",
    "* Load and save tabular data (CSV ↔ Parquet ↔ Excel) while comparing storage size, performance, and dtype preservation.\n",
    "* Normalize nested JSON into flat tables using `json_normalize`, ready for analysis.\n",
    "* Preprocess textual descriptions with TF-IDF to transform unstructured text into numerical features.\n",
    "* Extract image features using Local Binary Patterns (LBP) to represent visual texture numerically.\n",
    "\n",
    "By the end, you will know how to bring together heterogeneous data sources into one coherent analytical workflow, a key step toward building real, data-driven business insights.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9afb33e-d93c-4573-b4ca-5ddb4ce2fd33",
   "metadata": {},
   "source": [
    "# Part 1 — Tabular Data and Parquet Efficiency\n",
    "\n",
    "We start with a dataset that resembles a real business scenario: **Online Retail II**.  \n",
    "It contains transactions from a UK-based online store, including product descriptions, quantities, prices, customer IDs, and dates.  \n",
    "This dataset is ideal to study how pandas handles large tabular data and how Parquet improves efficiency in terms of size, speed, and type preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f3026-3c1f-4f18-8b73-1b2350a1499e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 1.0 — Download and Inspect the Data\n",
    "\n",
    "1. Download the **Online Retail II** dataset from the UCI Machine Learning Repository or Kaggle.  \n",
    "   You can use the following direct link for convenience:  \n",
    "   [https://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx](https://archive.ics.uci.edu/ml/machine-learning-databases/00502/online_retail_II.xlsx)  \n",
    "2. Save the file in the same folder that this notebook.  \n",
    "3. Verify that the file is available and readable by pandas.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfc538d3-9f0b-48b6-ae83-ed2c5e799717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "Path(\"online_retail_II.xlsx\").exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43beb1ca-a039-4075-8fe1-c85004210a11",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 1.1 — Load and Explore\n",
    "\n",
    "1. Read the CSV file into a DataFrame.  \n",
    "2. Display the first few rows.  \n",
    "3. Compute the total memory usage with `df.info(memory_usage=\"deep\")`.\n",
    "\n",
    "</div>\n",
    "\n",
    "Questions to consider:  \n",
    "* Which columns contain missing values?  \n",
    "* Which columns could be treated as categorical or datetime?  \n",
    "* How much memory does the dataset occupy?\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Tip:\n",
    "The dataset contains two sheets (2010–2011 and 2009–2010).\n",
    "You can choose one using the sheet_name argument in pd.read_excel, for example:\n",
    "\n",
    "`pd.read_excel(\"online_retail_II.xlsx\", sheet_name=\"Year 2010-2011\")`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f996c976-df98-42c2-88ef-fd1c7391a98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541910 entries, 0 to 541909\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   Invoice      541910 non-null  object        \n",
      " 1   StockCode    541910 non-null  object        \n",
      " 2   Description  540456 non-null  object        \n",
      " 3   Quantity     541910 non-null  int64         \n",
      " 4   InvoiceDate  541910 non-null  datetime64[ns]\n",
      " 5   Price        541910 non-null  float64       \n",
      " 6   Customer ID  406830 non-null  float64       \n",
      " 7   Country      541910 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 126.2 MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"online_retail_II.xlsx\", sheet_name=\"Year 2010-2011\")\n",
    "df.head()\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0245e14-96b1-4ab5-ae96-df1b07f6a901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 541910 entries, 0 to 541909\n",
      "Data columns (total 8 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   Invoice      541910 non-null  object        \n",
      " 1   StockCode    541910 non-null  object        \n",
      " 2   Description  540456 non-null  object        \n",
      " 3   Quantity     541910 non-null  int64         \n",
      " 4   InvoiceDate  541910 non-null  datetime64[ns]\n",
      " 5   Price        541910 non-null  float64       \n",
      " 6   Customer ID  406830 non-null  float64       \n",
      " 7   Country      541910 non-null  object        \n",
      "dtypes: datetime64[ns](1), float64(2), int64(1), object(4)\n",
      "memory usage: 126.2 MB\n"
     ]
    }
   ],
   "source": [
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a8d7d-3633-46a3-ab41-37a6ac3c0c5d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Exercise 1.2 — Clean and Prepare\n",
    "\n",
    "1. Convert InvoiceDate to datetime.\n",
    "2. Drop rows with missing Customer ID.\n",
    "3. Convert Country to a categorical variable.\n",
    "4. Recheck memory usage.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59180dd-fee8-4d6f-9ee1-f83d562edeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"], errors=\"coerce\")\n",
    "df = df.dropna(subset=[\"Customer ID\"])\n",
    "df[\"Country\"] = df[\"Country\"].astype(\"category\")\n",
    "df.info(memory_usage=\"deep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6485acb6-2f29-49b8-b733-9e2c97c7e651",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 1.3 — Write, Time, and Check Sizes\n",
    "\n",
    "1. Save the cleaned DataFrame as  \n",
    "   * CSV  \n",
    "   * Parquet with Snappy compression  \n",
    "   * Parquet with Gzip compression  \n",
    "2. Measure write times for each format using `%timeit`.  \n",
    "3. Check on disk file sizes and compare.\n",
    "\n",
    "</div\n",
    "\n",
    "\n",
    "Gzip often produces a smaller file than Snappy but takes longer to write.\n",
    "Snappy usually offers a better balance for analytics workflows.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Warning:\n",
    "If you see an error about missing engines for Parquet, install pyarrow in your environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739ddbc-4bdc-4cba-abd3-e78cce8b149e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "911c0818-2f0f-4b13-8f45-5e06afb0660c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "### Exercise 1.4 — Read, Time, and Check Memory\n",
    "\n",
    "1. Measure read times for CSV and both Parquet files using %timeit.\n",
    "2. After loading each version, call .info(memory_usage=\"deep\") and record total memory use.\n",
    "3. Confirm that row count and key dtypes match across formats.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "Tip:\n",
    "Parquet preserves categorical and datetime types more reliably than CSV.\n",
    "Inspect the `.dtypes` to confirm that `Country` is categorical and `InvoiceDate` is datetime after reading.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc95434-51f0-400b-91a9-5bfbb7d42dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ TIMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdb9bc-abf5-47eb-892e-4438f3830b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMORY USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049743de-f3e0-4982-b15d-0f25e3bbe3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INTEGRITY CHECKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfd5456-4c3c-4d00-a736-6f3d5dcb98f3",
   "metadata": {},
   "source": [
    "# Part 2 — Companies JSON Lines: Load, Normalize, Clean, and Export to Parquet\n",
    "\n",
    "You will work with a newline-delimited JSON file of company records.  \n",
    "Your goals are to load it correctly, normalize nested structures with `pd.json_normalize`, clean and standardize key fields, create a few compact features, validate, and export to Parquet.\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "**Expected result**  \n",
    "1. A companies table with one row per company and clean types  \n",
    "2. Normalized tables for offices, relationships, and funding rounds  \n",
    "3. An enriched companies table with simple aggregated features\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 2.0 — Download the JSON Lines file\n",
    "\n",
    "1. Open the dataset page  \n",
    "   https://github.com/ozlerhakan/mongodb-json-files/blob/master/datasets/companies.json  \n",
    "   Click Raw and save as `companies.json`.  \n",
    "2. Confirm the file is present before proceeding.\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3651cdc6-46c6-4cec-ad00-fa67e3dc4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "Path(\"companies.json\").exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bffba5-14a2-4538-9c0f-16fc0b0e9f64",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 2.1 — Load and quick schema audit\n",
    "\n",
    "1. Load the file using `pd.read_json(..., lines=True)`.  \n",
    "2. Inspect shape, columns, a few rows, and missingness.  \n",
    "3. Identify nested candidates for normalization such as `offices`, `relationships`, `funding_rounds`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Tip**  \n",
    "Newline-delimited JSON files contain one object per line.  Use `lines=True` to read them.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6cf6ff-62b3-4d4c-aea5-070ae9f73f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca996b-fabf-4436-a690-64abdeef3050",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 2.2 — Build a companies table with clean types\n",
    "\n",
    "1. Select useful top-level columns such as  \n",
    "   `permalink`, `name`, `category_code`, `founded_year`, `number_of_employees`, `deadpooled_year`, `total_money_raised`.  \n",
    "2. Convert years and employees to nullable integers.  \n",
    "3. Parse `total_money_raised` strings like `\"$5M\"` into numeric USD amounts.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e4d00b-ed5f-4ef4-87f1-d12d2eb3a9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def parse_money_usd(x):\n",
    "    if not isinstance(x, str) or not x:\n",
    "        return np.nan\n",
    "    s = x.strip().replace(\",\", \"\").replace(\"$\", \"\")\n",
    "    m = re.fullmatch(r\"(?i)(\\d*\\.?\\d+)\\s*([km]?)\", s)\n",
    "    if not m:\n",
    "        return np.nan\n",
    "    val = float(m.group(1))\n",
    "    suf = m.group(2).lower()\n",
    "    if suf == \"k\":\n",
    "        val *= 1_000\n",
    "    elif suf == \"m\":\n",
    "        val *= 1_000_000\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112b0d3f-4dd0-41ea-a6b8-5cea8f8fe276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e477633f-79ee-4f38-a57f-0d48c5667bc9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 2.3 — Normalize funding rounds with `pd.json_normalize`\n",
    "\n",
    "1. Expand the `funding_rounds` list so that each round becomes one row.  \n",
    "2. Keep the parent company keys with `meta=[\"permalink\",\"name\"]`.  \n",
    "3. Select and standardize key fields such as `round_code`, `funded_year`, `raised_amount`, `raised_currency_code`.  \n",
    "4. Coerce numeric fields and clean string columns.\n",
    "5. Select the following columns `[\"permalink\",\"name\",\"round_code\",\"funded_year\",\"raised_amount\",\"raised_currency_code\"]`.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "**Explanation — What `pd.json_normalize` is doing here**\n",
    "\n",
    "Each company record may contain a **list** under the key `funding_rounds`.  \n",
    "`pd.json_normalize(..., record_path=\"funding_rounds\", meta=[\"permalink\",\"name\"])` flattens that list so that:\n",
    "\n",
    "* Each funding round becomes **one row** in the output.  \n",
    "* `permalink` and `name` are copied from the parent company into every row to preserve the link.  \n",
    "* The result is a **rectangular table** with scalar columns that pandas can group, aggregate, and join easily.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2d107-e89e-4ce0-a570-e8f72142d773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "funding = pd.json_normalize(\n",
    "    df,\n",
    "    record_path=\"funding_rounds\",\n",
    "    meta=[\"permalink\",\"name\"]\n",
    ")\n",
    "\n",
    "funding = funding[[\n",
    "    \"permalink\",\"name\",\"round_code\",\n",
    "    \"funded_year\",\"raised_amount\",\"raised_currency_code\"\n",
    "]]\n",
    "\n",
    "funding[\"funded_year\"] = funding[\"funded_year\"].astype(\"Int64\")\n",
    "funding[\"raised_amount\"] = funding[\"raised_amount\"].astype(\"Float64\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0475a652-db52-4ac7-a9f5-4ae06158572d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 2.4 — Aggregate funding features and enrich the companies table\n",
    "\n",
    "1. Compute per company number of funding rounds, total raised amount for a chosen currency such as USD, and first and last funded years. Do it using a `.groupby` followed by a `.agg`. \n",
    "2. Merge these features back into the companies table using a left join on `permalink`.  \n",
    "3. Cast counts to nullable integers and amounts to float.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning**  \n",
    "If the dataset mixes currencies, aggregating all rounds without filtering can produce inconsistent totals.  \n",
    "Filter to a single currency or convert amounts before summing.\n",
    "\n",
    "</div>\n",
    "<div class=\"alert alert-danger\">\n",
    "\n",
    "**Pitfall**  \n",
    "Using an inner join when merging features would drop companies with no funding rounds.  \n",
    "Use a left join from the companies table to retain companies without funding and keep their feature values as missing.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c0f12-0d07-408d-a6f0-c1d9434b9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "agg = (\n",
    "    funding[funding[\"raised_currency_code\"] == \"USD\"]\n",
    "    .groupby(\"permalink\")\n",
    "    .agg(\n",
    "        rounds=(\"round_code\",\"count\"),\n",
    "        total_usd=(\"raised_amount\",\"sum\"),\n",
    "        first_year=(\"funded_year\",\"min\"),\n",
    "        last_year=(\"funded_year\",\"max\")\n",
    "    )\n",
    ")\n",
    "\n",
    "agg[\"rounds\"] = agg[\"rounds\"].astype(\"Int64\")\n",
    "agg[\"first_year\"] = agg[\"first_year\"].astype(\"Int64\")\n",
    "agg[\"last_year\"] = agg[\"last_year\"].astype(\"Int64\")\n",
    "\n",
    "companies_enriched = companies.merge(agg, on=\"permalink\", how=\"left\")\n",
    "companies_enriched.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3de640e-2dac-46a2-a325-79e8ac52e8fd",
   "metadata": {},
   "source": [
    "# Part 3 — Multimodal Data: Text + Images\n",
    "\n",
    "## Objective\n",
    "\n",
    "In this section, we will explore how to preprocess **multimodal data** — that is, data combining **textual descriptions** and **property images** — to make it ready for machine learning pipelines.  \n",
    "We will use a real dataset of property listings that contains both structured attributes and associated photos.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "### Learning goals\n",
    "\n",
    "By the end of these sections, you should be able to:\n",
    "\n",
    "1. Load and explore a dataset that includes both text and image data.  \n",
    "2. Preprocess listing descriptions for text analytics using **TF-IDF**.  \n",
    "3. Extract simple visual descriptors from images using **Local Binary Patterns (LBP)**.  \n",
    "4. Combine text, numeric, and image features into a single tabular format ready for modeling.\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 3.0 — Download the Real Estate dataset\n",
    "\n",
    "1. Open the dataset page  \n",
    "   https://huggingface.co/datasets/Binaryy/multimodal-real-estate-search  \n",
    "   This dataset contains both **property descriptions** and **images**.\n",
    "\n",
    "2. Make sure the **datasets** library is installed in your environment.\n",
    "\n",
    "3. Run the cell below to automatically download a small sample and prepare:\n",
    "- `real_estate/listings.csv` — text and metadata  \n",
    "- `real_estate/images/` — a few example property images\n",
    "\n",
    "4. Confirm the files are present before proceeding.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e6f5f2-8a17-4b2f-b337-b4148e8ef440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "root = Path(\"real_estate\")\n",
    "img_dir = root / \"images\"\n",
    "root.mkdir(exist_ok=True)\n",
    "img_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Load and sample 50 listings for fast execution\n",
    "ds = load_dataset(\"Binaryy/multimodal-real-estate-search\", split=\"train\").select(range(50))\n",
    "\n",
    "records = []\n",
    "for i, ex in enumerate(ds):\n",
    "    img = ex[\"image\"]\n",
    "    if isinstance(img, Image.Image):\n",
    "        path = img_dir / f\"{i:04d}.jpg\"\n",
    "        img.save(path)\n",
    "        records.append({\n",
    "            \"listing_id\": f\"L{i:04d}\",\n",
    "            \"city\": ex.get(\"Location\", \"\"),\n",
    "            \"description\": (ex.get(\"Title\", \"\") + \" \" + ex.get(\"Details\", \"\")).strip(),\n",
    "            \"image_file\": str(path)\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df.to_csv(root / \"listings.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} listings with images.\")\n",
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d34d102-7e3c-48f7-96a0-e387eb1ee224",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 3.1 — Create TF-IDF features from descriptions\n",
    "\n",
    "1. Load `real_estate/listings.csv`.  \n",
    "2. Use `TfidfVectorizer` to transform the text in the `description` column into numerical features.  \n",
    "3. Keep only the most frequent words and short word pairs (bigrams) to limit size.  \n",
    "4. Store the resulting matrix as a new DataFrame named `tfidf_df`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Tip**  \n",
    "Each column in `tfidf_df` now represents the importance of a specific word or bigram in a listing’s description.  \n",
    "You can later merge a subset of these features (for example, the top ones by variance) with numeric columns like price or city.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ae4d5-1cc8-4a59-95f8-ca045b57c24f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tfidf_24</th>\n",
       "      <th>tfidf_24 hours</th>\n",
       "      <th>tfidf_access</th>\n",
       "      <th>tfidf_accessories</th>\n",
       "      <th>tfidf_accessories modern</th>\n",
       "      <th>tfidf_ajah</th>\n",
       "      <th>tfidf_all</th>\n",
       "      <th>tfidf_all rooms</th>\n",
       "      <th>tfidf_also</th>\n",
       "      <th>tfidf_amenities</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_water heater</th>\n",
       "      <th>tfidf_water heaters</th>\n",
       "      <th>tfidf_we</th>\n",
       "      <th>tfidf_well</th>\n",
       "      <th>tfidf_whatsapp</th>\n",
       "      <th>tfidf_with</th>\n",
       "      <th>tfidf_with accessories</th>\n",
       "      <th>tfidf_with bq</th>\n",
       "      <th>tfidf_you</th>\n",
       "      <th>tfidf_your</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L0000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0001</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1427</td>\n",
       "      <td>0.087973</td>\n",
       "      <td>0.100295</td>\n",
       "      <td>0.150067</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106277</td>\n",
       "      <td>0.123706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130690</td>\n",
       "      <td>0.1427</td>\n",
       "      <td>0.136319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0002</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.092276</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0003</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.110230</td>\n",
       "      <td>0.125670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133165</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163754</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0004</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.146739</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074949</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            tfidf_24  tfidf_24 hours  tfidf_access  tfidf_accessories  \\\n",
       "listing_id                                                              \n",
       "L0000            0.0             0.0           0.0           0.000000   \n",
       "L0001            0.0             0.0           0.0           0.000000   \n",
       "L0002            0.0             0.0           0.0           0.000000   \n",
       "L0003            0.0             0.0           0.0           0.000000   \n",
       "L0004            0.0             0.0           0.0           0.146739   \n",
       "\n",
       "            tfidf_accessories modern  tfidf_ajah  tfidf_all  tfidf_all rooms  \\\n",
       "listing_id                                                                     \n",
       "L0000                       0.000000      0.0000   0.000000         0.000000   \n",
       "L0001                       0.000000      0.1427   0.087973         0.100295   \n",
       "L0002                       0.000000      0.0000   0.092276         0.105200   \n",
       "L0003                       0.000000      0.0000   0.110230         0.125670   \n",
       "L0004                       0.152258      0.0000   0.000000         0.000000   \n",
       "\n",
       "            tfidf_also  tfidf_amenities  ...  tfidf_water heater  \\\n",
       "listing_id                               ...                       \n",
       "L0000         0.000000           0.0000  ...            0.000000   \n",
       "L0001         0.150067           0.0000  ...            0.000000   \n",
       "L0002         0.000000           0.0000  ...            0.114933   \n",
       "L0003         0.000000           0.0000  ...            0.000000   \n",
       "L0004         0.000000           0.1417  ...            0.132773   \n",
       "\n",
       "            tfidf_water heaters  tfidf_we  tfidf_well  tfidf_whatsapp  \\\n",
       "listing_id                                                              \n",
       "L0000                  0.000000       0.0         0.0        0.000000   \n",
       "L0001                  0.000000       0.0         0.0        0.106277   \n",
       "L0002                  0.000000       0.0         0.0        0.111474   \n",
       "L0003                  0.188034       0.0         0.0        0.133165   \n",
       "L0004                  0.000000       0.0         0.0        0.000000   \n",
       "\n",
       "            tfidf_with  tfidf_with accessories  tfidf_with bq  tfidf_you  \\\n",
       "listing_id                                                                 \n",
       "L0000         0.068978                0.000000       0.000000     0.0000   \n",
       "L0001         0.123706                0.000000       0.130690     0.1427   \n",
       "L0002         0.000000                0.000000       0.000000     0.0000   \n",
       "L0003         0.077502                0.000000       0.163754     0.0000   \n",
       "L0004         0.074949                0.152258       0.000000     0.0000   \n",
       "\n",
       "            tfidf_your  \n",
       "listing_id              \n",
       "L0000         0.000000  \n",
       "L0001         0.136319  \n",
       "L0002         0.000000  \n",
       "L0003         0.000000  \n",
       "L0004         0.000000  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text = df[\"description\"].fillna(\"\")\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,\n",
    "    ngram_range=(1,2),\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    X.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "tfidf_df[\"listing_id\"] = df[\"listing_id\"].values\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16512705-39a5-4f3e-a5fb-f62be202772b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 4.1 — Build an LBP feature matrix for all images\n",
    "\n",
    "1. Load `real_estate/listings.csv`.  \n",
    "2. For each image, convert to grayscale, resize to a fixed shape, and compute **Local Binary Patterns (LBP)**.  \n",
    "3. Convert the grayscale image to `uint8` before applying LBP to avoid floating-point warnings.  \n",
    "4. Store the normalized LBP histogram for each image in a feature matrix and save it to Parquet.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Tip**  \n",
    "Converting to `uint8` ensures pixel values are discrete and stable for pattern comparisons.  \n",
    "With `P=8` and `method=\"uniform\"`, each row of `lbp_df` will have **10 histogram bins** (`P+2`).\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "**Warning**  \n",
    "Always resize to the same dimensions before feature extraction; differing resolutions change the texture statistics.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "\n",
    "**Tip**  \n",
    "To resize and convert an image to `uint8` safely:\n",
    "\n",
    "```python\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "\n",
    "img = rgb2gray(imread(\"real_estate/images/0000.jpg\"))\n",
    "img_resized = resize(img, (128, 128), anti_aliasing=True)\n",
    "img_uint8 = (img_resized * 255).astype(\"uint8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "43e81c40-310a-4937-a3ae-4ba008da579c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>lbp_6</th>\n",
       "      <th>lbp_7</th>\n",
       "      <th>lbp_8</th>\n",
       "      <th>lbp_9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>listing_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>L0000</th>\n",
       "      <td>0.046814</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>0.078735</td>\n",
       "      <td>0.086792</td>\n",
       "      <td>0.107544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0001</th>\n",
       "      <td>0.033813</td>\n",
       "      <td>0.067566</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.214661</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>0.072876</td>\n",
       "      <td>0.088074</td>\n",
       "      <td>0.093811</td>\n",
       "      <td>0.099426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0002</th>\n",
       "      <td>0.035278</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.105652</td>\n",
       "      <td>0.226074</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>0.076294</td>\n",
       "      <td>0.072815</td>\n",
       "      <td>0.096130</td>\n",
       "      <td>0.097351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0003</th>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.103394</td>\n",
       "      <td>0.246643</td>\n",
       "      <td>0.166382</td>\n",
       "      <td>0.048950</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.111450</td>\n",
       "      <td>0.090210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L0004</th>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>0.288879</td>\n",
       "      <td>0.232361</td>\n",
       "      <td>0.062012</td>\n",
       "      <td>0.066956</td>\n",
       "      <td>0.064026</td>\n",
       "      <td>0.061768</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               lbp_0     lbp_1     lbp_2     lbp_3     lbp_4     lbp_5  \\\n",
       "listing_id                                                               \n",
       "L0000       0.046814  0.076416  0.055725  0.110535  0.209351  0.161133   \n",
       "L0001       0.033813  0.067566  0.045410  0.110535  0.214661  0.173828   \n",
       "L0002       0.035278  0.084900  0.043396  0.105652  0.226074  0.162109   \n",
       "L0003       0.019714  0.085449  0.025208  0.103394  0.246643  0.166382   \n",
       "L0004       0.013977  0.043396  0.032288  0.134338  0.288879  0.232361   \n",
       "\n",
       "               lbp_6     lbp_7     lbp_8     lbp_9  \n",
       "listing_id                                          \n",
       "L0000       0.066956  0.078735  0.086792  0.107544  \n",
       "L0001       0.072876  0.088074  0.093811  0.099426  \n",
       "L0002       0.076294  0.072815  0.096130  0.097351  \n",
       "L0003       0.048950  0.102600  0.111450  0.090210  \n",
       "L0004       0.062012  0.066956  0.064026  0.061768  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13590ea-a5de-49f6-adf8-58a30d4ef1a3",
   "metadata": {},
   "source": [
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Exercise 4.2 — Merge text (TF-IDF) + image (LBP) + listings\n",
    "\n",
    "1. Ensure you have already created:\n",
    "   - `df` from `real_estate/listings.csv`\n",
    "   - `tfidf_df` from Exercise 3.1\n",
    "   - `lbp_df` from Exercise 4.1 (and saved to `real_estate/exports/lbp_features.parquet`)\n",
    "2. Add `listing_id` to `tfidf_df` and merge everything on `listing_id`.\n",
    "3. Save the final multimodal table to Parquet.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f75c22db-2e95-48c0-887d-4a94e93a96a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (50, 231)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>city</th>\n",
       "      <th>description</th>\n",
       "      <th>image_file</th>\n",
       "      <th>lbp_0</th>\n",
       "      <th>lbp_1</th>\n",
       "      <th>lbp_2</th>\n",
       "      <th>lbp_3</th>\n",
       "      <th>lbp_4</th>\n",
       "      <th>lbp_5</th>\n",
       "      <th>...</th>\n",
       "      <th>tfidf_water heater</th>\n",
       "      <th>tfidf_water heaters</th>\n",
       "      <th>tfidf_we</th>\n",
       "      <th>tfidf_well</th>\n",
       "      <th>tfidf_whatsapp</th>\n",
       "      <th>tfidf_with</th>\n",
       "      <th>tfidf_with accessories</th>\n",
       "      <th>tfidf_with bq</th>\n",
       "      <th>tfidf_you</th>\n",
       "      <th>tfidf_your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L0000</td>\n",
       "      <td>Lekki Phase 1, Lekki, Lagos</td>\n",
       "      <td>5 bedroom detached duplex for sale FOR SALE\\n\\...</td>\n",
       "      <td>real_estate/images/0000.jpg</td>\n",
       "      <td>0.046814</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>0.055725</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.161133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068978</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L0001</td>\n",
       "      <td>Contemporary 5 Bedroom Detached Duplex With Bq...</td>\n",
       "      <td>5 bedroom detached duplex for sale Newly built...</td>\n",
       "      <td>real_estate/images/0001.jpg</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>0.067566</td>\n",
       "      <td>0.045410</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.214661</td>\n",
       "      <td>0.173828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.106277</td>\n",
       "      <td>0.123706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.130690</td>\n",
       "      <td>0.1427</td>\n",
       "      <td>0.136319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L0002</td>\n",
       "      <td>In A Fully Organized Estate, Lekki Phase 1, Le...</td>\n",
       "      <td>4 bedroom block of flats for sale FOR SALE : L...</td>\n",
       "      <td>real_estate/images/0002.jpg</td>\n",
       "      <td>0.035278</td>\n",
       "      <td>0.084900</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.105652</td>\n",
       "      <td>0.226074</td>\n",
       "      <td>0.162109</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.111474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L0003</td>\n",
       "      <td>Off Freedom Way, Lekki Phase 1, Lekki, Lagos</td>\n",
       "      <td>6 bedroom detached duplex for sale DETACHED DU...</td>\n",
       "      <td>real_estate/images/0003.jpg</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.085449</td>\n",
       "      <td>0.025208</td>\n",
       "      <td>0.103394</td>\n",
       "      <td>0.246643</td>\n",
       "      <td>0.166382</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188034</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.133165</td>\n",
       "      <td>0.077502</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.163754</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L0004</td>\n",
       "      <td>Lekki Phase 1, Lekki, Lagos</td>\n",
       "      <td>3 bedroom terraced duplex for sale BRAND NEW 3...</td>\n",
       "      <td>real_estate/images/0004.jpg</td>\n",
       "      <td>0.013977</td>\n",
       "      <td>0.043396</td>\n",
       "      <td>0.032288</td>\n",
       "      <td>0.134338</td>\n",
       "      <td>0.288879</td>\n",
       "      <td>0.232361</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074949</td>\n",
       "      <td>0.152258</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  listing_id                                               city  \\\n",
       "0      L0000                        Lekki Phase 1, Lekki, Lagos   \n",
       "1      L0001  Contemporary 5 Bedroom Detached Duplex With Bq...   \n",
       "2      L0002  In A Fully Organized Estate, Lekki Phase 1, Le...   \n",
       "3      L0003       Off Freedom Way, Lekki Phase 1, Lekki, Lagos   \n",
       "4      L0004                        Lekki Phase 1, Lekki, Lagos   \n",
       "\n",
       "                                         description  \\\n",
       "0  5 bedroom detached duplex for sale FOR SALE\\n\\...   \n",
       "1  5 bedroom detached duplex for sale Newly built...   \n",
       "2  4 bedroom block of flats for sale FOR SALE : L...   \n",
       "3  6 bedroom detached duplex for sale DETACHED DU...   \n",
       "4  3 bedroom terraced duplex for sale BRAND NEW 3...   \n",
       "\n",
       "                    image_file     lbp_0     lbp_1     lbp_2     lbp_3  \\\n",
       "0  real_estate/images/0000.jpg  0.046814  0.076416  0.055725  0.110535   \n",
       "1  real_estate/images/0001.jpg  0.033813  0.067566  0.045410  0.110535   \n",
       "2  real_estate/images/0002.jpg  0.035278  0.084900  0.043396  0.105652   \n",
       "3  real_estate/images/0003.jpg  0.019714  0.085449  0.025208  0.103394   \n",
       "4  real_estate/images/0004.jpg  0.013977  0.043396  0.032288  0.134338   \n",
       "\n",
       "      lbp_4     lbp_5  ...  tfidf_water heater  tfidf_water heaters  tfidf_we  \\\n",
       "0  0.209351  0.161133  ...            0.000000             0.000000       0.0   \n",
       "1  0.214661  0.173828  ...            0.000000             0.000000       0.0   \n",
       "2  0.226074  0.162109  ...            0.114933             0.000000       0.0   \n",
       "3  0.246643  0.166382  ...            0.000000             0.188034       0.0   \n",
       "4  0.288879  0.232361  ...            0.132773             0.000000       0.0   \n",
       "\n",
       "   tfidf_well  tfidf_whatsapp  tfidf_with  tfidf_with accessories  \\\n",
       "0         0.0        0.000000    0.068978                0.000000   \n",
       "1         0.0        0.106277    0.123706                0.000000   \n",
       "2         0.0        0.111474    0.000000                0.000000   \n",
       "3         0.0        0.133165    0.077502                0.000000   \n",
       "4         0.0        0.000000    0.074949                0.152258   \n",
       "\n",
       "   tfidf_with bq  tfidf_you  tfidf_your  \n",
       "0       0.000000     0.0000    0.000000  \n",
       "1       0.130690     0.1427    0.136319  \n",
       "2       0.000000     0.0000    0.000000  \n",
       "3       0.163754     0.0000    0.000000  \n",
       "4       0.000000     0.0000    0.000000  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
